<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>publications | Xiao  Li</title>
    <meta name="author" content="Xiao  Li">
    <meta name="description" content="">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700%7CRoboto+Slab:100,300,400,500,700%7CMaterial+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    <!-- Styles -->
    
    <link rel="shortcut icon" href="/assets/img/icon2.png">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="www.xiao-li.org/publications/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span style="font-family:Helvetica;font-weight:50">Xiao </span><span style="font-family:Helvetica;font-weight:50">Li</span></a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">home</a>
              </li>


              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/teaching/">teaching</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/research/">research</a>
              </li>
              <li class="nav-item active">
                <a class="nav-link" href="/publications/">publications<span class="sr-only">(current)</span></a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">

              </li>
            </ul>
          </div>
        </div>
      </nav>

    </header>


    <!-- Content -->
    <div class="container mt-5">
      <!-- page.html -->
        <div class="post">

          <!-- <header class="post-header">
            <h1 class="post-title">publications</h1>
            <p class="post-description"></p>
          </header> -->

          <article>
            <p>A selected collection of research outcomes. This <a href="https://scholar.google.com/citations?user=WkRojboAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Google Scholar</a> page contains full publications.</p>

<p><br></p>
<h2 id="selected-articles-by-topic"><span style="font-family:Helvetica;font-weight:150;font-size:40px">Selected Articles (by topic)</span></h2>

<p><br></p>
<h2 id="topic-llms-and-optimization-for-llms"><span style="font-family:Helvetica;font-weight:100;font-size:25px">Topic: LLMs and Optimization for LLMs</span></h2>

<!-- 2025 StreamBP -->
<p><span style="font-family:Arial;font-weight:50;font-size:15.5px"> StreamBP: Memory-Efficient Exact Backpropagation for Long Sequence Training of LLMs. </span>   <br> Qijun Luo, Mengqi Li, Lei Zhao, Xiao Li. <br> Advances in Neural Information Processing Systems (NeurIPS 2025).   [<a href="https://github.com/Ledzy/StreamBP" rel="external nofollow noopener" target="_blank">Github Repo</a>]   [<a href="https://arxiv.org/abs/2506.03077" rel="external nofollow noopener" target="_blank">arXiv Link</a>]
<br> 
<span style="color:grey;font-size:14px"> &gt;&gt;&gt; StreamBP is an algorithm that implements memory efficient and exact Backpropagation for training LLMs on ultra long sequence (e.g., training reasoning model) or for scaling up batch sizes. It is both memory and time efficient compared to standard BP with grad ckpt.</span></p>

<!-- 2025 BREAD -->
<p><span style="font-family:Arial;font-weight:50;font-size:15.5px"> Accelerating Block Coordinate Descent for LLM Finetuning via Landscape Expansion. </span>   <br> Qijun Luo, Yifei Shen, Liangzu Peng, Dongsheng Li, Xiao Li. <br> Advances in Neural Information Processing Systems (NeurIPS 2025). <br>
Github repo and paper link will be available soon.
<br> 
<span style="color:grey;font-size:14px"> &gt;&gt;&gt; We identify two limitations of BCD for LLM training: It wastes some intermediate gradients and has a narrow search direction. We solves the issues simultaneously by incorporating the lightweight optimization techniques such as SGD and LoRA. </span></p>

<!-- 2025 DPO-Shift -->
<!-- <span style="font-family:Arial;font-weight:50;font-size:15.5px"> DPO-Shift: Shifting the Distribution of Direct Preference Optimization. </span>   <br/> Xiliang Yang, Feng Jiang, Qianen Zhang, Lei Zhao, Xiao Li. <br/> [[Github Repo](https://github.com/Meaquadddd/DPO-Shift)] &nbsp; [[arXiv Preprint](https://arxiv.org/abs/2502.07599)]
<br/> 
<span style="color:grey;font-size:14px"> \>\>\> DPO-shift mitigates the likelihood displacement issue of DPO through a simple approach. It is theoretically grounded. Thorough experiments illustrate that DPO-shift is effective on various models and datasets. </span>  -->

<!-- 2024 BAdam -->
<p><span style="font-family:Arial;font-weight:50;font-size:15.5px"> BAdam: A Memory Efficient Full Parameter Optimization Method for Large Language Models. </span>   <br> Qijun Luo, Hengxu Yu, Xiao Li. <br> Advances in Neural Information Processing Systems (NeurIPS 2024).   [<a href="https://openreview.net/pdf?id=0uXtFk5KNJ" rel="external nofollow noopener" target="_blank">NeurIPS Link</a>]   [<a href="https://github.com/Ledzy/BAdam" rel="external nofollow noopener" target="_blank">Github Repo</a>]   [<a href="https://drive.google.com/file/d/13LraIpwIwFF4_0CWo7L1uMaRyMcg4Veg/view?usp=sharing" rel="external nofollow noopener" target="_blank">Slides</a>]
<br> 
<span style="color:grey;font-size:14px"> &gt;&gt;&gt; BAdam is a block coordinate descent optimization method with Adam’s update rule for finetuning large language models. It is memory efficient and allows training Llama 3-8B using a single RTX3090-24GB GPU and training Llama 3-70B using 4\(\times\)A100. </span></p>

<p><br></p>
<h2 id="topic-stochastic-optimization"><span style="font-family:Helvetica;font-weight:100;font-size:25px">Topic: Stochastic Optimization</span></h2>

<!-- 2023 RR HighProb -->
<p><span style="font-family:Arial;font-weight:50;font-size:15.5px"> High Probability Guarantees for Random Reshuffling. </span>   <br> Hengxu Yu, Xiao Li. <br> [<a href="https://arxiv.org/abs/2311.11841" rel="external nofollow noopener" target="_blank">arXiv Preprint</a>]
<br> 
<span style="color:grey;font-size:14px"> &gt;&gt;&gt; We estabilished a set of high probability finite-time compleixity guarantees for RR, including finding a stationary point, desinging a stopping criterion that yields the last iterate result, and avoding saddle points. </span></p>

<!-- 2023 RR at SIOPT -->
<p><span style="font-family:Arial;font-weight:50;font-size:15.5px"> Convergence of Random Reshuffling Under The Kurdyka-Lojasiewicz Inequality. </span>   <br> Xiao Li, Andre Milzarek, Junwen Qiu. <br> SIAM Journal on Optimization, 33(2), 1092-1120, 2023.    [<a href="https://epubs.siam.org/doi/10.1137/21M1468048" rel="external nofollow noopener" target="_blank">SIAM Link</a>]   [<a href="https://arxiv.org/abs/2110.04926" target="_blank" rel="external nofollow noopener">arXiv</a>]   [<a href="https://drive.google.com/file/d/10vtq6wqTAy2nBPrhArqnZb_V1YGpoGdR/view?usp=sharing" target="_blank" rel="external nofollow noopener">Slides</a>] 
<br> 
<span style="color:grey;font-size:14px"> &gt;&gt;&gt; The sequence convergence results are established for random reshuffling (stochastic). The key insights are: 1) derive subsequence convergence using diminishing step sizes and 2) combine diminishing step sizes with the traditional KL analysis. </span></p>

<!-- 2022 unified convergence theorem at NIPS -->
<p><span style="font-family:Arial;font-weight:50;font-size:15.5px">  A Unified Convergence Theorem for Stochastic Optimization Methods. </span>   <br> Xiao Li, Andre Milzarek. <br> Advances in Neural Information Processing Systems (NeurIPS 2022).    [<a href="https://openreview.net/forum?id=muvlhVKvd4" target="_blank" rel="external nofollow noopener">NeurIPS Link</a>]   [<a href="https://drive.google.com/file/d/1H2pHzWdvvLir-0v3oOq_1SM0Tlw1hRjY/view?usp=share_link" target="_blank" rel="external nofollow noopener">Slides</a>] 
<br> 
<span style="color:grey;font-size:14px"> &gt;&gt;&gt; In this work, we provide a fundamental convergence theorem and apply it to obtain almost sure convergence results for SGD, RR, prox-SGD, and stochastic model-based methods. </span></p>

<p><br></p>
<h2 id="topic-nonsmooth-andor-nonconvex-optimization"><span style="font-family:Helvetica;font-weight:100;font-size:25px">Topic: Nonsmooth and/or Nonconvex Optimization</span></h2>

<!-- 2024 SubGrad_NonLips -->
<p><span style="font-family:Arial;font-weight:50;font-size:15.5px"> Revisiting Subgradient Method: Complexity and Convergence Beyond Lipschitz Continuity. </span>   <br> Xiao Li, Lei Zhao, Daoli Zhu, Anthony Man-Cho So. <br> Vietnam Journal of Mathematics, 2024.    [<a href="https://link.springer.com/article/10.1007/s10013-024-00715-w" rel="external nofollow noopener" target="_blank">Springer Link</a>]   [<a href="https://arxiv.org/abs/2305.14161" rel="external nofollow noopener" target="_blank">arXiv</a>]
<br> 
(Invited article dedicated to Prof. Tamás Terlaky on the occasion of his 70th birthday)
<br> 
<span style="color:grey;font-size:14px"> &gt;&gt;&gt; The subgradient method and some of its varaints possess convergence properties without Lipschitz continuity at all. </span></p>

<!-- 2023 ReSync -->
<p><span style="font-family:Arial;font-weight:50;font-size:15.5px"> ReSync: Riemannian Subgradient-based Robust Rotation Synchronization. </span>   <br> Huikang Liu, Xiao Li, Anthony Man-Cho So.
<br>  Advances in Neural Information Processing Systems (NeurIPS 2023).   [<a href="https://openreview.net/pdf?id=Xxllzjt6T5" rel="external nofollow noopener" target="_blank">NeurIPS Link</a>]
<br> 
<span style="color:grey;font-size:14px"> &gt;&gt;&gt; We present ReSync, a Riemannian subgradient-based method for solving the nonconvex nonsmooth robust rotation synchronization problem, and provide linear convergence guarantees for ReSync in terms of finding the underlying rotations.  </span></p>

<!-- 2021 RSG at SIOPT -->
<p><span style="font-family:Arial;font-weight:50;font-size:15.5px"> Weakly Convex Optimization over Stiefel Manifold Using Riemannian Subgradient-Type Methods. </span>   <br> Xiao Li, Shixiang Chen, Zengde Deng, Qing Qu, Zhihui Zhu, Anthony Man Cho So. <br> SIAM Journal on Optimization, 31(3), 1605–1634, 2021.   [<a href="https://epubs.siam.org/doi/pdf/10.1137/20M1321000" target="_blank" rel="external nofollow noopener">SIAM Link</a>]   [<a href="https://arxiv.org/abs/1911.05047" target="_blank" rel="external nofollow noopener">arXiv</a>]
<br> 
<span style="color:grey;font-size:14px"> &gt;&gt;&gt; We provide the first complexity/convergence results for Riemannian subgradient-type methods. The key insight is that weakly convex function restricted on smooth embedded manifold is still  weakly convex. </span></p>

<!-- 2020 incremental methods at OPT -->
<p><span style="font-family:Arial;font-weight:50;font-size:15.5px"> Incremental Methods for Weakly Convex Optimization. </span>   <br> Xiao Li, Zhihui Zhu, Anthony Man-Cho So, Jason D Lee. <br> NeurIPS 2020 Workshop (OPT 2020).   [<a href="https://arxiv.org/abs/1907.11687" target="_blank" rel="external nofollow noopener">arXiv</a>]
<br> 
<span style="color:grey;font-size:14px"> &gt;&gt;&gt; We analyzed the incremental subgradient, proximal point, and prox-linear methods. The typical \(\mathcal{O}(\varepsilon^{-4})\) compelexity and local linear rate (with sharpness conditoin) are established. </span></p>

<!-- 2020 RMS at SIOPT -->
<p><span style="font-family:Arial;font-weight:50;font-size:15.5px"> Nonconvex Robust Low-rank Matrix Recovery. </span>   <br> Xiao Li, Zhihui Zhu, Anthony Man-Cho So, Rene Vidal. <br> SIAM Journal on Optimization, 30(1), 660–686, 2020.   [<a href="https://epubs.siam.org/doi/abs/10.1137/18M1224738?mobileUi=0" target="_blank" rel="external nofollow noopener">SIAM Link</a>]   [<a href="https://arxiv.org/abs/1809.09237" target="_blank" rel="external nofollow noopener">arXiv</a>]   [<a href="https://github.com/lixiao0982/Nonconvex-Robust-Low-rank-Matrix-Recovery" target="_blank" rel="external nofollow noopener">Code</a>]
<br> 
<span style="color:grey;font-size:14px"> &gt;&gt;&gt; We propose the robust nonconvex matrix recovery problem, and show that subgradient method will linearly converge to optima. The key insight is the recovery condition “\(\ell_1/\ell_2\)-RIP’’ leads to optimization properties — weak convexity and sharpness.  </span></p>


          </article>

        </div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">

    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script defer src="/assets/js/common.js"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

  </body>
</html>
