<!-- ---
layout: page
permalink: /publications/
title: publications
description: 
nav: true
nav_order: 3
--- -->


A selected collection of publications. This [Google Scholar](https://scholar.google.com/citations?user=WkRojboAAAAJ&hl=en) page contains more publications.




<!-- Add Preprints -->
<br/>
## <span style="font-family:Helvetica;font-weight:100;font-size:30px">Preprints</span>

<!-- 2024 norm-PRR -->
<span style="font-family:Arial;font-weight:50;font-size:15.5px"> A New Random Reshuffling Method for Nonsmooth Nonconvex Finite-sum Optimization. </span>   <br/> Junwen Qiu, Xiao Li, Andre Milzarek. <br/> [[arXiv Preprint](https://arxiv.org/abs/2312.01047)]
<br/> 
<span style="color:grey;font-size:14px"> \>\>\> We propose a new debiased proximal random reshuffling method for nonsmooth regularized nonconvex problems. We establish similar complexity and convergence results to those of smooth random reshuffling method. </span>

<!-- 2023 RR HighProb -->
<span style="font-family:Arial;font-weight:50;font-size:15.5px"> High Probability Guarantees for Random Reshuffling. </span>   <br/> Hengxu Yu, Xiao Li. <br/> [[arXiv Preprint](https://arxiv.org/abs/2311.11841)]
<br/> 
<span style="color:grey;font-size:14px"> \>\>\> We estabilished a set of high probability finite-time compleixity guarantees for RR, including finding a stationary point, desinging a stopping criterion that yields the last iterate result, and avoding saddle points. </span>

<!-- 2023 RCS -->
<!-- <span style="font-family:Arial;font-weight:50;font-size:15.5px"> Randomized Coordinate Subgradient Method for Nonsmooth Composite Optimization. </span>   <br/> Lei Zhao, Ding Chen, Daoli Zhu, Xiao Li. <br/> [[arXiv Preprint](https://arxiv.org/abs/2206.14981)]
<br/> 
<span style="color:grey;font-size:14px"> \>\>\> The first coordinate-type subgradient method for weakly convex and nonsmooth composite functions. </span> -->





<!-- Add Published Journla and Conference Articles -->
<!-- 2023 SubGrad -->

<br/>
## <span style="font-family:Helvetica;font-weight:100;font-size:30px">Refereed Articles</span> 

<!-- 2024 BAdam -->
<span style="font-family:Arial;font-weight:50;font-size:15.5px"> BAdam: A Memory Efficient Full Parameter Optimization Method for Large Language Models. </span>   <br/> Qijun Luo, Hengxu Yu, Xiao Li. <br/> Advances in Neural Information Processing Systems (NeurIPS 2024). &nbsp; [[NeurIPS Link](https://openreview.net/pdf?id=0uXtFk5KNJ)] &nbsp; [[Github Repo](https://github.com/Ledzy/BAdam)] &nbsp; [[Slides](https://drive.google.com/file/d/13LraIpwIwFF4_0CWo7L1uMaRyMcg4Veg/view?usp=sharing)]
<br/> 
<span style="color:grey;font-size:14px"> \>\>\> BAdam is a block coordinate descent optimization method with Adam's update rule for finetuning large language models. It is memory efficient and allows training Llama 3-8B using a single RTX3090-24GB GPU and training Llama 3-70B using 4$$\times$$A100. </span> 



<!-- 2024 SubGrad_NonLips -->
<span style="font-family:Arial;font-weight:50;font-size:15.5px"> Revisiting Subgradient Method: Complexity and Convergence Beyond Lipschitz Continuity. </span>   <br/> Xiao Li, Lei Zhao, Daoli Zhu, Anthony Man-Cho So. <br/> Vietnam Journal of Mathematics, 2024.  &nbsp; [[Springer Link](https://link.springer.com/article/10.1007/s10013-024-00715-w)] &nbsp; [[arXiv](https://arxiv.org/abs/2305.14161)]
<br/> 
(Invited article dedicated to Prof. Tamás Terlaky on the occasion of his 70th birthday)
<br/> 
<span style="color:grey;font-size:14px"> \>\>\> The subgradient method and some of its varaints possess convergence properties without Lipschitz continuity at all. </span>

<!-- 2024 DSO general variance -->
<span style="font-family:Arial;font-weight:50;font-size:15.5px"> Distributed Stochastic Optimization under a General Variance Condition. </span>   <br/> Kun Huang, Xiao Li, Shi Pu. 
<br/> IEEE Transactions on Automatic Control, 69(9), 6105 - 6120, 2024.  &nbsp; [[IEEE Link](https://ieeexplore.ieee.org/document/10508084)] &nbsp; [[arXiv](https://arxiv.org/abs/2301.12677)]
<br/> 
<span style="color:grey;font-size:14px"> \>\>\> We established the convergence of FedAvg using a general variance condition, providing an informative measrue for charactering data heterogeneity. Interestingly, we revealed that SCAFFOLD also suffers from data heterogeneity under this general variance condition. </span>

<!-- 2023 ReSync -->
<span style="font-family:Arial;font-weight:50;font-size:15.5px"> ReSync: Riemannian Subgradient-based Robust Rotation Synchronization. </span>   <br/> Huikang Liu, Xiao Li, Anthony Man-Cho So.
<br/>  Advances in Neural Information Processing Systems (NeurIPS 2023). &nbsp; [[NeurIPS Link](https://openreview.net/pdf?id=Xxllzjt6T5)]
<br/> 
<span style="color:grey;font-size:14px"> \>\>\> We present ReSync, a Riemannian subgradient-based method for solving the nonconvex nonsmooth robust rotation synchronization problem, and provide linear convergence guarantees for ReSync in terms of finding the underlying rotations.  </span>

<!-- 2023 DRR TSP -->
<span style="font-family:Arial;font-weight:50;font-size:15.5px"> Distributed Random Reshuffling over Networks. </span>   <br/> Kun Huang, Xiao Li, Andre Milzarek, Shi Pu, Junwen Qiu. <br/> IEEE Transactions on Signal Processing, 71, 1143-1158, 2023. &nbsp; [[IEEE Link](https://ieeexplore.ieee.org/document/10081450)] &nbsp; [[arXiv](https://arxiv.org/abs/2112.15287)] 
<br/> 
<span style="color:grey;font-size:14px"> \>\>\> We design a simple yet powerful decentralized random reshuffling method over networks. Similar complexity and convergence guarantees to the centralized random reshuffling method are established. </span>

<!-- 2023 single-timescale AC at TMLR -->
<span style="font-family:Arial;font-weight:50;font-size:15.5px"> Finite-Time Analysis of Decentralized Single-Timescale Actor-Critic. </span>   <br/> Qijun Luo, Xiao Li. <br/> Transactions on Machine Learning Research, 2023. &nbsp; [[TMLR Link](https://openreview.net/pdf?id=KQRv0O8iW4)] &nbsp; [<a href="https://arxiv.org/abs/2206.05733" target="_blank">arXiv</a>]
<br/> 
<span style="color:grey;font-size:14px"> \>\>\> We showed the $$\tilde {\mathcal O}(\varepsilon^{-2})$$ sample complexity for single-timescale AC algorithm (previous results are based on either double-loop or two-timescale scheme).  The key step is to estalish the smoothness condition of the optimal critic variable. </span>

<!-- 2023 RR at SIOPT -->
<span style="font-family:Arial;font-weight:50;font-size:15.5px"> Convergence of Random Reshuffling Under The Kurdyka-Lojasiewicz Inequality. </span>   <br/> Xiao Li, Andre Milzarek, Junwen Qiu. <br/> SIAM Journal on Optimization, 33(2), 1092-1120, 2023.  &nbsp; [[SIAM Link](https://epubs.siam.org/doi/10.1137/21M1468048)] &nbsp; [<a href="https://arxiv.org/abs/2110.04926" target="_blank">arXiv</a>] &nbsp; [<a href="https://drive.google.com/file/d/10vtq6wqTAy2nBPrhArqnZb_V1YGpoGdR/view?usp=sharing" target="_blank">Slides</a>] 
<br/> 
<span style="color:grey;font-size:14px"> \>\>\> The sequence convergence results are established for random reshuffling (stochastic). The key insights are: 1) derive subsequence convergence using diminishing step sizes and 2) combine diminishing step sizes with the traditional KL analysis. </span>

<!-- 2022 unified convergence theorem at NIPS -->
<span style="font-family:Arial;font-weight:50;font-size:15.5px">  A Unified Convergence Theorem for Stochastic Optimization Methods. </span>   <br/> Xiao Li, Andre Milzarek. <br/> Advances in Neural Information Processing Systems (NeurIPS 2022).  &nbsp; [<a href="https://openreview.net/forum?id=muvlhVKvd4" target="_blank">NeurIPS Link</a>] &nbsp; [<a href="https://drive.google.com/file/d/1H2pHzWdvvLir-0v3oOq_1SM0Tlw1hRjY/view?usp=share_link" target="_blank">Slides</a>] 
<br/> 
<span style="color:grey;font-size:14px"> \>\>\> In this work, we provide a fundamental convergence theorem and apply it to obtain almost sure convergence results for SGD, RR, prox-SGD, and stochastic model-based methods. </span> 

<!-- 2021 SNMF at TKDE -->
<span style="font-family:Arial;font-weight:50;font-size:15.5px"> A Provable Splitting Approach for Symmetric Nonnegative Matrix Factorization. </span>   <br/> Xiao Li, Zhihui Zhu, Qiuwei Li, Kai Liu. <br/> IEEE Transactions on Knowledge and Data Engineering, 35(3), 2206-2219, 2023. &nbsp; [<a href="https://ieeexplore.ieee.org/document/9606619" target="_blank">IEEE Link</a>]  &nbsp; [[arXiv]](https://arxiv.org/abs/2301.10499)
<br/> 
<span style="color:grey;font-size:14px"> \>\>\> After reformulating the symmetric NMF as a nonsymmetric penalized version, we design a set of alternating algorithms and provide a unified framework for analyzing their convergence by modifying the existing KL analysis. </span> 

<!-- 2021 RSG at SIOPT -->
<span style="font-family:Arial;font-weight:50;font-size:15.5px"> Weakly Convex Optimization over Stiefel Manifold Using Riemannian Subgradient-Type Methods. </span>   <br/> Xiao Li, Shixiang Chen, Zengde Deng, Qing Qu, Zhihui Zhu, Anthony Man Cho So. <br/> SIAM Journal on Optimization, 31(3), 1605–1634, 2021. &nbsp; [<a href="https://epubs.siam.org/doi/pdf/10.1137/20M1321000" target="_blank">SIAM Link</a>] &nbsp; [<a href="https://arxiv.org/abs/1911.05047" target="_blank">arXiv</a>]
<br/> 
<span style="color:grey;font-size:14px"> \>\>\> We provide the first complexity/convergence results for Riemannian subgradient-type methods. The key insight is that weakly convex function restricted on smooth embedded manifold is still  weakly convex. </span> 

<!-- 2020 incremental methods at OPT -->
<!-- <span style="font-family:Arial;font-weight:50;font-size:15.5px"> Incremental Methods for Weakly Convex Optimization. </span>   <br/> Xiao Li, Zhihui Zhu, Anthony Man-Cho So, Jason D Lee. <br/> NeurIPS 2020 Workshop (OPT 2020). &nbsp; [<a href="https://arxiv.org/abs/1907.11687" target="_blank">arXiv</a>]
<br/> 
<span style="color:grey;font-size:14px"> \>\>\> We analyzed the incremental subgradient, proximal point, and prox-linear methods. The typical $$\mathcal{O}(\varepsilon^{-4})$$ compelexity and local linear rate (with sharpness conditoin) are established. </span>  -->

<!-- 2020 sparse blind deconvolution at SMIIS -->
<span style="font-family:Arial;font-weight:50;font-size:15.5px"> Exact Recovery of Multichannel Sparse Blind Deconvolution via Gradient Descent. </span>   <br/> Qing Qu, Xiao Li, Zhihui Zhu. <br/> SIAM Journal on Imaging Sciences, 13(3), 1630-1652, 2020. &nbsp; [<a href="https://epubs.siam.org/doi/abs/10.1137/19M1291327" target="_blank">SIAM Link</a>]
<br/> 
<span style="color:grey;font-size:14px"> \>\>\> We show that the multi-channel sparse blind deconvolution problem has a local strong convexity-like property, which yields strong global convergence property to the optimal/underlying solution of gradient descent. </span> 

<!-- 2020 overcomplete learning at ICLR -->
<span style="font-family:Arial;font-weight:50;font-size:15.5px"> Geometric Analysis of Nonconvex Optimization Landscapes for Overcomplete Learning. </span>   <br/> Qing Qu, Yuexiang Zhai, Xiao Li, Yuqian Zhang, Zhihui Zhu. <br/> International Conference on Learning Representation (ICLR 2020). &nbsp; [<a href="https://openreview.net/forum?id=rygixkHKDH" target="_blank">ICLR Link</a>] 
<br/> 
<span style="color:grey;font-size:14px"> \>\>\> We consider a $$\ell_4$$ maximization problem over the sphere for learning sparsely used overcomplete dictionaries. We show this problem has benigh global geometry so that simple local search algorithms are guaranteed to find the global solution. </span> 

<!-- 2020 RMS at SIOPT -->
<span style="font-family:Arial;font-weight:50;font-size:15.5px"> Nonconvex Robust Low-rank Matrix Recovery. </span>   <br/> Xiao Li, Zhihui Zhu, Anthony Man-Cho So, Rene Vidal. <br/> SIAM Journal on Optimization, 30(1), 660–686, 2020. &nbsp; [<a href="https://epubs.siam.org/doi/abs/10.1137/18M1224738?mobileUi=0" target="_blank">SIAM Link</a>] &nbsp; [<a href="https://arxiv.org/abs/1809.09237" target="_blank">arXiv</a>] &nbsp; [<a href="https://github.com/lixiao0982/Nonconvex-Robust-Low-rank-Matrix-Recovery" target="_blank">Code</a>]
<br/> 
<span style="color:grey;font-size:14px"> \>\>\> We propose the robust nonconvex matrix recovery problem, and show that subgradient method will linearly converge to optima. The key insight is the recovery condition "$$\ell_1/\ell_2$$-RIP'' leads to optimization properties --- weak convexity and sharpness.  </span> 

<!-- 2019 sparse blind deconvolution at NIPS -->
<span style="font-family:Arial;font-weight:50;font-size:15.5px"> A Nonconvex Approach for Exact and Efficient Multichannel Sparse Blind Deconvolution. </span>   <br/> Qing Qu, Xiao Li, Zhihui Zhu. <br/> Advances in Neural Information Processing Systems (NeurIPS 2019). &nbsp; [<a href="http://papers.nips.cc/paper/8656-a-nonconvex-approach-for-exact-and-efficient-multichannel-sparse-blind-deconvolution" target="_blank">NeurIPS Link</a>]
<br/> 
<span style="color:grey;font-size:14px"> \>\>\> We give a comprehensive geometric characterization for the Multichannel Sparse Blind Deconvolution on the sphere, including a large gradient area, a benign area, and a perturbation area.  </span> 

<!-- 2019 sparse sensing at SP -->
<!-- <span style="font-family:Arial;font-weight:50;font-size:15.5px"> Optimized Structured Sparse Sensing Matrices for Compressive Sensing. </span>   <br/> Tao Hong, Xiao Li, Zhihui Zhu, Qiuwei Li. <br/> Signal Processing, 159, 119-129, 2019.  &nbsp; [<a href="https://www.sciencedirect.com/science/article/pii/S0165168419300532" target="_blank">Elsevier Link</a>] &nbsp; [<a href="https://arxiv.org/abs/1709.06895" target="_blank">arXiv</a>] &nbsp; [<a href="https://github.com/lixiao0982/Optimized-Sparse-Sensing-Matrix-" target="_blank">Code</a>] -->

<!-- 2018 SNMF at NIPS -->
<span style="font-family:Arial;font-weight:50;font-size:15.5px"> Dropping Symmetry for Fast Symmetric Nonnegative Matrix Factorization. </span>   <br/> Zhihui Zhu, Xiao Li, Kai Liu, Qiuwei Li. <br/> Advances in Neural Information Processing Systems (NeurIPS 2018). &nbsp; [<a href="http://papers.nips.cc/paper/7762-dropping-symmetry-for-fast-symmetric-nonnegative-matrix-factorization" target="_blank">NeurIPS Link</a>] &nbsp; [<a href="https://github.com/lixiao0982/Dropping-Symmetric-for-Symmetric-NMF" target="_blank">Code</a>]
<br/> 
<span style="color:grey;font-size:14px"> \>\>\> We formulate a nonsymmetric penalized version for the symmetric NMF and show that solving the nonsymmetric version returns a solution for the symmetric one, thus transfering symmetric NMF to a nonsymmetric one theoretically.  </span> 

<!-- 2015 robust sensing matrix at TIP -->
<span style="font-family:Arial;font-weight:50;font-size:15.5px"> Designing Robust Sensing Matrix for Image Compression. </span>   <br/> Gang Li, Xiao Li, Sheng Li, Huang Bai, Qianru Jiang, Xiongxiong He. <br/> IEEE Transactions on Image Processing, 24(12), 5389-5400, 2015. &nbsp; [<a href="http://ieeexplore.ieee.org/document/7272112/" target="_blank">IEEE Link</a>]
<br/> 
<span style="color:grey;font-size:14px"> \>\>\> We consider the error matrix arising in the sparse representation, and merge it into the sensing matrix design procedure in compressed sensing.  </span> 







<!-- Add Technical Reports -->
<!-- <br/>
## <span style="font-family:Helvetica;font-weight:100;font-size:30px">Technical Reports</span>  -->


<!-- alternating projection -->
<!-- <span style="font-family:Arial;font-weight:50;font-size:15.5px"> Convergence Analysis of Alternating Projection Method for Nonconvex Sets. </span>   <br/> Zhihui Zhu, Xiao Li. <br/> Technical Report. &nbsp; [<a href="https://arxiv.org/abs/1802.03889" target="_blank">arXiv</a>] -->


<!-- Sparse subspace -->

<!-- <span style="font-family:Arial;font-weight:50;font-size:15.5px"> Finding the Sparsest Vectors in a Subspace: Theory, Algorithms, and Applications. </span>   <br/> Qing Qu, Zhihui Zhu, Xiao Li, Manolis C. Tsakiris, John Wright, Rene Vidal. <br/> Technical Report. &nbsp; [<a href="https://arxiv.org/abs/2001.06970" target="_blank">arXiv</a>] -->