<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>research | Xiao  Li</title>
    <meta name="author" content="Xiao  Li">
    <meta name="description" content="A growing collection of your cool projects.">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700%7CRoboto+Slab:100,300,400,500,700%7CMaterial+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    <!-- Styles -->
    
    <link rel="shortcut icon" href="/assets/img/icon2.png">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="www.xiao-li.org/research/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span style="font-family:Helvetica;font-weight:50">Xiao </span><span style="font-family:Helvetica;font-weight:50">Li</span></a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">home</a>
              </li>


              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/teaching/">teaching</a>
              </li>
              <li class="nav-item active">
                <a class="nav-link" href="/research/">research<span class="sr-only">(current)</span></a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/seminar/">seminar</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">

              </li>
            </ul>
          </div>
        </div>
      </nav>

    </header>


    <!-- Content -->
    <div class="container mt-5">
      <!-- page.html -->
        <div class="post">

          <!-- <header class="post-header">
            <h1 class="post-title">research</h1>
            <p class="post-description">A growing collection of your cool projects.</p>
          </header> -->

          <article>
            <p>My research interests lie in several fundamental aspects of optimization and machine learning. They are often motivated by practical needs and/or observations. Here are currently ongoing research projects of my group.</p>

<!-- Project: RR -->
<h2 id="design-and-analysis-of-stochastic-optimization-methods"><span style="font-family:Helvetica;font-weight:100;font-size:25px">Design and Analysis of Stochastic Optimization Methods</span></h2>
<p>Stochastic gradient method (SGD) plays an important role in modern machine learning due to the large-scale feature of the arising problems. Most of the (supervised) learning problem amounts to solving an emperical risk minimization problem, i.e., minimizing a finite-sum objective \(f(w) = \frac{1}{n}\sum_{i = 1}^n f_i(w)\). There are several variants of SGD for solving this type of problem.  In the theory side, the existing analyses mainly apply to the following i.i.d. sampling version of SGD:</p>

\[w_{t+1} = w_t - \alpha_t \nabla f_{i_t} (w_t).\]

<p>Here, \(i_t\) is sampled from \(\{1,\ldots, n\}\) <em>uniformly at random</em> and every sampling is independent from the previous ones. Though such a SGD scheme is well known and widely analyzed, a quick criticism of this scheme is that it may not visit all the data points in one epoch, resulting in inefficency in practice.  What is instead often implemented in practice (in PyTorch) is the so-called <em>random reshuffling</em> (RR), which in each epoch randomly <em>permutes</em> \(\{1,\ldots, n\}\) (denoted by \(\sigma_t\)) and updates by sweeping the elements of \(\sigma_t\) sequentially. That is, \(i_t\) incrementally takes value in \(\sigma_t\). It is easy to see that such a method will have a much smaller sampling variance and will utilize all the data points in one epoch. In practice, one also uses adaptive step szie (learning rate) and momentum for stablizing and accelerating training, leading to the well known method Adam. These stochastic optimization methods and their variants serve as the fundamental solvers of modern machine learning problems.</p>

<p>Understanding the properties of these methods are of practical importance, which is a major direction of this project. Antoher important direction is to design new stochastic optimization methods for better performance.</p>

<p>Here are some related papers of this project:</p>

<p><span style="font-family:Arial;font-weight:50;font-size:15.5px"> Convergence of Random Reshuffling Under The Kurdyka-Lojasiewicz Inequality. </span>   <br> Xiao Li, Andre Milzarek, Junwen Qiu. <br> SIAM Journal on Optimization, 33(2), 1092-1120, 2023.    [<a href="https://epubs.siam.org/doi/10.1137/21M1468048" rel="external nofollow noopener" target="_blank">SIAM Link</a>]   [<a href="https://arxiv.org/abs/2110.04926" target="_blank" rel="external nofollow noopener">arXiv</a>]   [<a href="https://drive.google.com/file/d/10vtq6wqTAy2nBPrhArqnZb_V1YGpoGdR/view?usp=sharing" target="_blank" rel="external nofollow noopener">Slides</a>]</p>

<p><span style="font-family:Arial;font-weight:50;font-size:15.5px"> Distributed Random Reshuffling over Networks. </span>   <br> Kun Huang, Xiao Li, Andre Milzarek, Shi Pu, Junwen Qiu. <br> IEEE Transactions on Signal Processing, 71, 1143-1158, 2023.   [<a href="https://ieeexplore.ieee.org/document/10081450" rel="external nofollow noopener" target="_blank">IEEE Link</a>]   [<a href="https://arxiv.org/abs/2112.15287" rel="external nofollow noopener" target="_blank">arXiv</a>]</p>

<p><span style="font-family:Arial;font-weight:50;font-size:15.5px">  A Unified Convergence Theorem for Stochastic Optimization Methods. </span>   <br> Xiao Li, Andre Milzarek. <br> Advances in Neural Information Processing Systems (NeurIPS 2022).    [<a href="https://openreview.net/forum?id=muvlhVKvd4" target="_blank" rel="external nofollow noopener">NeurIPS Link</a>]   [<a href="https://drive.google.com/file/d/1H2pHzWdvvLir-0v3oOq_1SM0Tlw1hRjY/view?usp=share_link" target="_blank" rel="external nofollow noopener">Slides</a>]</p>

<!-- Project: AC -->
<!-- ## <span style="font-family:Helvetica;font-weight:100;font-size:25px">Actor Critic in Reinforcement Learning </span>
Actor Critic (AC) is a popular algorithmic framework for policy-based reinforcement learning. It uses parametrization for both policy (with parameter $$\theta$$) and value function (with parameter $$w$$), and hence transfers the reward maximization task into an optimization task over $$\theta$$ and $$w$$. In this algorithm, the actor model (i.e. policy) maximizes the objective function using (stochastic) gradient ascent, where the stochastic gradient is calculated using the value estimated by a critic model. 

In practice, AC ususally implements a *single-timescale* update scheme, namely, it adopts an alternating optimization framework over $$\theta$$ and $$w$$, and update each variable at every iteration for constant number of steps using the same order of step sizes for $$\theta$$ and $$w$$. However, most of the exisitng analyses focus on  either a *double-loop* or a *two-timescale* framework. Therefore, the aim of this project is to understand the properties of single-timescale AC and designing its variants with applications to practical policy optimization problems. 

The following is a starting point of this project:

<span style="font-family:Arial;font-weight:50;font-size:15.5px"> Finite-Time Analysis of Decentralized Single-Timescale Actor-Critic. </span>   <br/> Qijun Luo, Xiao Li. <br/> To appear in Transactions on Machine Learning Research, 2023. &nbsp; [<a href="https://arxiv.org/abs/2206.05733" target="_blank">arXiv</a>] -->

<!-- Project: almost sure -->
<!-- ## <span style="font-family:Helvetica;font-weight:100;font-size:25px"> Almost Sure Convergence Properties of Stochastic Optimization Methods </span>
In the past years, we often care about nonasymptotic properties for stochastic optimization methods. Suppose we are using SGD to optimize a smooth nonconvex function $$f$$, its finite-time complexity guarantee often has the form

$$ \min_{k=0,\ldots, T} \ \mathbb{E}[\|\nabla f(x^k)\|^2] \leq \mathcal{O} \left(\frac{1}{\sqrt{T+1}}\right) \quad \text{or} \quad \mathbb{E}[\|\nabla f(x^{\bar k})\|^2] \leq \mathcal{O} \left(\frac{1}{\sqrt{T+1}}\right) $$

where $$T$$ denotes the total number of iterations and $$\bar k$$ is an index sampled uniformly at random from $$\{0,\ldots, T\}$$.  Though such a result does tell us some meaningful properties of SGD in the first $$T$$ iterations, it has three main drawbacks: 1) It holds only *in expectation*. Mathematically, this describes the behavior of SGD by averaging infinitely many runs. 2) It is not for the *last iterate*. 3) It tells little the *eventual behavior* of SGD as one cannot let $$T\to\infty$$ in the above equation. 

However,  in practical situations the algorithm is often only run once and the last iterate is returned as a solution. This indicates that the above complexity result does not fully characterize the properties of SGD. This project is to study the *almost sure* eventual behavior of stochastic methods. Togetehr with the typical complexity results in expectation, we will be able to give a more complete picture for the properties of SGD. 

The following is a starting point of this project:

<span style="font-family:Arial;font-weight:50;font-size:15.5px">  A Unified Convergence Theorem for Stochastic Optimization Methods. </span>   <br/> Xiao Li, Andre Milzarek. <br/> Neural Information Processing Systems (NeurIPS 2022).  &nbsp; [<a href="https://openreview.net/forum?id=muvlhVKvd4" target="_blank">NeurIPS Link</a>]

<span style="font-family:Arial;font-weight:50;font-size:15.5px"> Convergence of Random Reshuffling Under The Kurdyka-Lojasiewicz Inequality. </span>   <br/> Xiao Li, Andre Milzarek, Junwen Qiu. <br/> To appear in SIAM Journal on Optimization, 2023.  &nbsp; [<a href="https://arxiv.org/abs/2110.04926" target="_blank">arXiv</a>] -->

<!-- Project: Nonconvex optimization and algorithm -->
<h2 id="-nonconvex-optimization-and-algorithm-design-"><span style="font-family:Helvetica;font-weight:100;font-size:25px"> Nonconvex Optimization and Algorithm Design </span></h2>
<p>In general, nonconvex optimization is not feasible for attaining global optimality. There are a set of nonconvex problems in machine learning and signal processing that exhibit benign properties. For instance, a fundamental problem is to recover \(w^*\) from \(y = Xw^* + e\), given \(y\) and \(X\). We omit the dimension here for simplicity. Note that the lower case \(y\), \(w^*\), and \(e\) are not restricted to be vectors, they can also be matrices.  Such a problem has various names in different communities by imposing different structures on \(w^*\). A family of good examples are variants of (robust) PCA when \(w^*\) is assume to be a low-rank matrix. In this project, we consider both the analysis of the nonconvex optimization problem landscape and the design of efficient algorithms.</p>

<!-- 
Typical recovery guarantees were for $$\ell_2$$ metric that is used for measuring how good the fitting is. This $$\ell_2$$ metric approach implicitly assumes that the error $$e$$ is modeled as Gaussian. However, we might have *outliers* (non-Gaussian) in practical measurement process. This motivates us to consider more robust approachs / metrics for recovering $$w^*$$, which is the main topic of this project. The specific nonconvex robust recovery problems we will consider includes Robust PCA variants, rotation averaging, etc.  -->

<p>Here are some outcomes along this line of research:</p>

<p><span style="font-family:Arial;font-weight:50;font-size:15.5px"> ReSync: Riemannian Subgradient-based Robust Rotation Synchronization. </span>   <br> Huikang Liu, Xiao Li, Anthony Man-Cho So.
<br>  Advances in Neural Information Processing Systems (NeurIPS 2023).   [<a href="https://arxiv.org/abs/2305.15136" rel="external nofollow noopener" target="_blank">arXiv</a>]</p>

<p><span style="font-family:Arial;font-weight:50;font-size:15.5px"> Weakly Convex Optimization over Stiefel Manifold Using Riemannian Subgradient-Type Methods. </span>   <br> Xiao Li, Shixiang Chen, Zengde Deng, Qing Qu, Zhihui Zhu, Anthony Man Cho So. <br> SIAM Journal on Optimization, 31(3), 1605–1634, 2021.   [<a href="https://arxiv.org/abs/1911.05047" target="_blank" rel="external nofollow noopener">arXiv</a>]</p>

<p><span style="font-family:Arial;font-weight:50;font-size:15.5px"> Exact Recovery of Multichannel Sparse Blind Deconvolution via Gradient Descent. </span>   <br> Qing Qu, Xiao Li, Zhihui Zhu. <br> SIAM Journal on Imaging Sciences, 13(3), 1630-1652, 2020.   [<a href="https://epubs.siam.org/doi/abs/10.1137/19M1291327" target="_blank" rel="external nofollow noopener">SIAM Link</a>]</p>

<p><span style="font-family:Arial;font-weight:50;font-size:15.5px"> Nonconvex Robust Low-rank Matrix Recovery. </span>   <br> Xiao Li, Zhihui Zhu, Anthony Man-Cho So, Rene Vidal. <br> SIAM Journal on Optimization, 30(1), 660–686, 2020.   [<a href="https://arxiv.org/abs/1809.09237" target="_blank" rel="external nofollow noopener">arXiv</a>]</p>

<p><span style="font-family:Arial;font-weight:50;font-size:15.5px"> A Nonconvex Approach for Exact and Efficient Multichannel Sparse Blind Deconvolution. </span>   <br> Qing Qu, Xiao Li, Zhihui Zhu. <br> Neural Information Processing Systems (NeurIPS 2019).   [<a href="http://papers.nips.cc/paper/8656-a-nonconvex-approach-for-exact-and-efficient-multichannel-sparse-blind-deconvolution" target="_blank" rel="external nofollow noopener">NeurIPS Link</a>]</p>

          </article>

        </div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">

    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script defer src="/assets/js/common.js"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

  </body>
</html>
